{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load('imdb_reviews', with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = imdb['train']\n",
    "test_data = imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_GeneratorState', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_add_variable_with_custom_getter', '_apply_options', '_as_serialized_graph', '_as_variant_tensor', '_checkpoint_dependencies', '_component_metadata', '_consumers', '_dataset', '_deferred_dependencies', '_flat_shapes', '_flat_structure', '_flat_types', '_from_components', '_functions', '_gather_saveables_for_checkpoint', '_graph', '_graph_attr', '_handle_deferred_dependencies', '_has_captured_ref', '_inputs', '_is_graph_tensor', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_make_initializable_iterator', '_make_one_shot_iterator', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_preload_simple_restoration', '_restore_from_checkpoint_position', '_self_name_based_restores', '_self_setattr_tracking', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_setattr_tracking', '_shape_invariant_to_type_spec', '_single_restoration_from_checkpoint_position', '_tf_api_names', '_tf_api_names_v1', '_to_components', '_trace_variant_creation', '_track_trackable', '_tracking_metadata', '_type_spec', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', '_variant_tensor', '_variant_tensor_attr', '_variant_tracker', 'apply', 'batch', 'cache', 'concatenate', 'element_spec', 'enumerate', 'filter', 'filter_with_legacy_function', 'flat_map', 'from_generator', 'from_sparse_tensor_slices', 'from_tensor_slices', 'from_tensors', 'interleave', 'list_files', 'make_initializable_iterator', 'make_one_shot_iterator', 'map', 'map_with_legacy_function', 'options', 'output_classes', 'output_shapes', 'output_types', 'padded_batch', 'prefetch', 'range', 'reduce', 'repeat', 'shard', 'shuffle', 'skip', 'take', 'unbatch', 'window', 'with_options', 'zip']\n"
     ]
    }
   ],
   "source": [
    "print(dir(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=199, shape=(), dtype=string, numpy=b'This is a big step down after the surprisingly enjoyable original. This sequel isn\\'t nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is toned down in this entry. The performances are all adequate, but this time the script lets us down. The action is merely routine and the plot is only mildly interesting, so I need lots of silly laughs in order to stay entertained during a \"Trancers\" movie. Unfortunately, the laughs are few and far between, and so, this film is watchable at best.'>, <tf.Tensor: id=200, shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.__iter__()\n",
    "print(next(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=201, shape=(), dtype=string, numpy=b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it since I was in seventh grade at a Presbyterian school, so I am not sure what effect it would have on me now. However, I will say that it left an impression on me... and most of my friends. It did serve its purpose, at least until we were old enough and knowledgeable enough to analyze and create our own opinions. I was particularly terrified of what the newly-converted post-rapture Christians had to endure when not receiving the mark of the beast. I don't want to spoil the movie for those who haven't seen it so I will not mention details of the scenes, but I can still picture them in my head... and it's been 19 years.\">, <tf.Tensor: id=202, shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "print(next(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven\\'t seen it since I was in seventh grade at a Presbyterian school, so I am not sure what effect it would have on me now. However, I will say that it left an impression on me... and most of my friends. It did serve its purpose, at least until we were old enough and knowledgeable enough to analyze and create our own opinions. I was particularly terrified of what the newly-converted post-rapture Christians had to endure when not receiving the mark of the beast. I don\\'t want to spoil the movie for those who haven\\'t seen it so I will not mention details of the scenes, but I can still picture them in my head... and it\\'s been 19 years.\"'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_sentences\n",
    "training_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_labels\n",
    "training_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_f = np.array(training_labels)\n",
    "training_labels_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels_f = np.array(testing_labels)\n",
    "testing_labels_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "oov_tok = '<OOV>'\n",
    "max_len = 120\n",
    "trunc = 'post'\n",
    "embedding_dim =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences , maxlen = max_len, truncating= trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "test_padded = pad_sequences(test_sequences , maxlen = max_len, truncating= trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key,value) in word_index.items()])\n",
    "#reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  59,  390,   87,   11,   14,   37,  185, 1377,    3,    1,   55,\n",
       "         11,  216,   10,   12,   18,   14,    2, 1217,    5,  112,    1,\n",
       "       3587,   16,   73,   11, 1449,  111,   10,  235,   11,   14,    9,\n",
       "       8986, 1257,   32,    4,    1,  396,   37,   11,  240,   24,  250,\n",
       "         51,  970,   10,   64,   28,   22,   73,  150,  190,   11,   81,\n",
       "        136,   13,   10,  318,   35, 1404,   22,   73,    3,   92,    5,\n",
       "         63,  373,   10,  123, 2935,   94, 1302,   32,  219,  374,   75,\n",
       "         72,  154,  194,    3,    1,  194,    6, 8659,    3,  995,  262,\n",
       "        203, 4754,   11,   14,  584, 5682,    5,   51,    2, 4755, 9330,\n",
       "       1218, 9163, 4635,   71,    6, 4376,   55,   24, 5618,    2,  955,\n",
       "          5,    2, 2751,   11,  174,  182,    6, 2392,    2,   18])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b perhaps because i was so young innocent and <OOV> when i saw it this movie was the cause of many <OOV> nights for me i haven't seen it since i was in seventh grade at a <OOV> school so i am not sure what effect it would have on me now however i will say that it left an impression on me and most of my friends it did serve its purpose at least until we were old enough and <OOV> enough to analyze and create our own opinions i was particularly terrified of what the newly converted post rapture christians had to endure when not receiving the mark of the beast i don't want to spoil the movie\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(padded[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven\\'t seen it since I was in seventh grade at a Presbyterian school, so I am not sure what effect it would have on me now. However, I will say that it left an impression on me... and most of my friends. It did serve its purpose, at least until we were old enough and knowledgeable enough to analyze and create our own opinions. I was particularly terrified of what the newly-converted post-rapture Christians had to endure when not receiving the mark of the beast. I don\\'t want to spoil the movie for those who haven\\'t seen it so I will not mention details of the scenes, but I can still picture them in my head... and it\\'s been 19 years.\"'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_len),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation ='relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics =['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - ETA: 9:10 - loss: 0.6910 - accuracy: 0.56 - ETA: 36s - loss: 0.6918 - accuracy: 0.5254 - ETA: 18s - loss: 0.6928 - accuracy: 0.514 - ETA: 12s - loss: 0.6920 - accuracy: 0.517 - ETA: 9s - loss: 0.6919 - accuracy: 0.518 - ETA: 8s - loss: 0.6919 - accuracy: 0.51 - ETA: 7s - loss: 0.6914 - accuracy: 0.51 - ETA: 6s - loss: 0.6906 - accuracy: 0.51 - ETA: 5s - loss: 0.6898 - accuracy: 0.52 - ETA: 5s - loss: 0.6879 - accuracy: 0.52 - ETA: 4s - loss: 0.6871 - accuracy: 0.52 - ETA: 4s - loss: 0.6845 - accuracy: 0.54 - ETA: 3s - loss: 0.6810 - accuracy: 0.55 - ETA: 3s - loss: 0.6760 - accuracy: 0.56 - ETA: 3s - loss: 0.6714 - accuracy: 0.57 - ETA: 3s - loss: 0.6658 - accuracy: 0.58 - ETA: 2s - loss: 0.6593 - accuracy: 0.59 - ETA: 2s - loss: 0.6528 - accuracy: 0.60 - ETA: 2s - loss: 0.6454 - accuracy: 0.61 - ETA: 2s - loss: 0.6386 - accuracy: 0.62 - ETA: 2s - loss: 0.6308 - accuracy: 0.63 - ETA: 2s - loss: 0.6237 - accuracy: 0.63 - ETA: 2s - loss: 0.6142 - accuracy: 0.64 - ETA: 1s - loss: 0.6070 - accuracy: 0.65 - ETA: 1s - loss: 0.6003 - accuracy: 0.65 - ETA: 1s - loss: 0.5946 - accuracy: 0.66 - ETA: 1s - loss: 0.5880 - accuracy: 0.66 - ETA: 1s - loss: 0.5831 - accuracy: 0.67 - ETA: 1s - loss: 0.5767 - accuracy: 0.67 - ETA: 1s - loss: 0.5699 - accuracy: 0.68 - ETA: 1s - loss: 0.5660 - accuracy: 0.68 - ETA: 1s - loss: 0.5602 - accuracy: 0.69 - ETA: 1s - loss: 0.5548 - accuracy: 0.69 - ETA: 1s - loss: 0.5500 - accuracy: 0.69 - ETA: 1s - loss: 0.5455 - accuracy: 0.70 - ETA: 1s - loss: 0.5407 - accuracy: 0.70 - ETA: 0s - loss: 0.5381 - accuracy: 0.70 - ETA: 0s - loss: 0.5346 - accuracy: 0.71 - ETA: 0s - loss: 0.5308 - accuracy: 0.71 - ETA: 0s - loss: 0.5263 - accuracy: 0.71 - ETA: 0s - loss: 0.5236 - accuracy: 0.72 - ETA: 0s - loss: 0.5200 - accuracy: 0.72 - ETA: 0s - loss: 0.5162 - accuracy: 0.72 - ETA: 0s - loss: 0.5140 - accuracy: 0.72 - ETA: 0s - loss: 0.5117 - accuracy: 0.72 - ETA: 0s - loss: 0.5097 - accuracy: 0.73 - ETA: 0s - loss: 0.5061 - accuracy: 0.73 - ETA: 0s - loss: 0.5035 - accuracy: 0.73 - ETA: 0s - loss: 0.5008 - accuracy: 0.73 - ETA: 0s - loss: 0.4983 - accuracy: 0.73 - ETA: 0s - loss: 0.4956 - accuracy: 0.74 - 4s 176us/sample - loss: 0.4939 - accuracy: 0.7429 - val_loss: 0.3836 - val_accuracy: 0.8281\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - ETA: 7s - loss: 0.1554 - accuracy: 1.00 - ETA: 2s - loss: 0.2175 - accuracy: 0.92 - ETA: 2s - loss: 0.2359 - accuracy: 0.91 - ETA: 2s - loss: 0.2328 - accuracy: 0.91 - ETA: 2s - loss: 0.2343 - accuracy: 0.91 - ETA: 2s - loss: 0.2396 - accuracy: 0.91 - ETA: 2s - loss: 0.2476 - accuracy: 0.90 - ETA: 2s - loss: 0.2512 - accuracy: 0.90 - ETA: 2s - loss: 0.2486 - accuracy: 0.90 - ETA: 2s - loss: 0.2441 - accuracy: 0.90 - ETA: 2s - loss: 0.2418 - accuracy: 0.91 - ETA: 1s - loss: 0.2410 - accuracy: 0.91 - ETA: 1s - loss: 0.2414 - accuracy: 0.90 - ETA: 1s - loss: 0.2406 - accuracy: 0.91 - ETA: 1s - loss: 0.2418 - accuracy: 0.91 - ETA: 1s - loss: 0.2422 - accuracy: 0.91 - ETA: 1s - loss: 0.2399 - accuracy: 0.91 - ETA: 1s - loss: 0.2376 - accuracy: 0.91 - ETA: 1s - loss: 0.2386 - accuracy: 0.91 - ETA: 1s - loss: 0.2369 - accuracy: 0.91 - ETA: 1s - loss: 0.2383 - accuracy: 0.91 - ETA: 1s - loss: 0.2374 - accuracy: 0.91 - ETA: 1s - loss: 0.2376 - accuracy: 0.91 - ETA: 1s - loss: 0.2377 - accuracy: 0.91 - ETA: 1s - loss: 0.2372 - accuracy: 0.91 - ETA: 1s - loss: 0.2376 - accuracy: 0.91 - ETA: 1s - loss: 0.2385 - accuracy: 0.91 - ETA: 1s - loss: 0.2391 - accuracy: 0.91 - ETA: 1s - loss: 0.2389 - accuracy: 0.91 - ETA: 1s - loss: 0.2397 - accuracy: 0.90 - ETA: 1s - loss: 0.2395 - accuracy: 0.90 - ETA: 0s - loss: 0.2391 - accuracy: 0.90 - ETA: 0s - loss: 0.2397 - accuracy: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2395 - accuracy: 0.90 - ETA: 0s - loss: 0.2392 - accuracy: 0.90 - ETA: 0s - loss: 0.2383 - accuracy: 0.90 - ETA: 0s - loss: 0.2397 - accuracy: 0.90 - ETA: 0s - loss: 0.2406 - accuracy: 0.90 - ETA: 0s - loss: 0.2406 - accuracy: 0.90 - ETA: 0s - loss: 0.2410 - accuracy: 0.90 - ETA: 0s - loss: 0.2414 - accuracy: 0.90 - ETA: 0s - loss: 0.2423 - accuracy: 0.90 - ETA: 0s - loss: 0.2416 - accuracy: 0.90 - ETA: 0s - loss: 0.2423 - accuracy: 0.90 - ETA: 0s - loss: 0.2430 - accuracy: 0.90 - ETA: 0s - loss: 0.2434 - accuracy: 0.90 - ETA: 0s - loss: 0.2438 - accuracy: 0.90 - ETA: 0s - loss: 0.2433 - accuracy: 0.90 - ETA: 0s - loss: 0.2429 - accuracy: 0.90 - 3s 138us/sample - loss: 0.2431 - accuracy: 0.9066 - val_loss: 0.4202 - val_accuracy: 0.8165\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - ETA: 6s - loss: 0.0940 - accuracy: 0.96 - ETA: 2s - loss: 0.1098 - accuracy: 0.98 - ETA: 2s - loss: 0.1113 - accuracy: 0.97 - ETA: 2s - loss: 0.1050 - accuracy: 0.98 - ETA: 2s - loss: 0.1054 - accuracy: 0.98 - ETA: 2s - loss: 0.1056 - accuracy: 0.98 - ETA: 2s - loss: 0.1064 - accuracy: 0.98 - ETA: 2s - loss: 0.1026 - accuracy: 0.98 - ETA: 2s - loss: 0.1043 - accuracy: 0.98 - ETA: 2s - loss: 0.1029 - accuracy: 0.98 - ETA: 2s - loss: 0.1026 - accuracy: 0.97 - ETA: 2s - loss: 0.1024 - accuracy: 0.97 - ETA: 2s - loss: 0.1017 - accuracy: 0.97 - ETA: 2s - loss: 0.1004 - accuracy: 0.97 - ETA: 1s - loss: 0.0991 - accuracy: 0.97 - ETA: 1s - loss: 0.0989 - accuracy: 0.97 - ETA: 1s - loss: 0.1009 - accuracy: 0.97 - ETA: 1s - loss: 0.1002 - accuracy: 0.97 - ETA: 1s - loss: 0.1012 - accuracy: 0.97 - ETA: 1s - loss: 0.1008 - accuracy: 0.97 - ETA: 1s - loss: 0.1003 - accuracy: 0.97 - ETA: 1s - loss: 0.0998 - accuracy: 0.97 - ETA: 1s - loss: 0.0992 - accuracy: 0.97 - ETA: 1s - loss: 0.0985 - accuracy: 0.97 - ETA: 1s - loss: 0.0984 - accuracy: 0.97 - ETA: 1s - loss: 0.0985 - accuracy: 0.97 - ETA: 1s - loss: 0.0988 - accuracy: 0.97 - ETA: 1s - loss: 0.0996 - accuracy: 0.97 - ETA: 1s - loss: 0.1003 - accuracy: 0.97 - ETA: 1s - loss: 0.0994 - accuracy: 0.97 - ETA: 1s - loss: 0.0988 - accuracy: 0.97 - ETA: 1s - loss: 0.0992 - accuracy: 0.97 - ETA: 1s - loss: 0.0988 - accuracy: 0.97 - ETA: 1s - loss: 0.0986 - accuracy: 0.97 - ETA: 1s - loss: 0.0980 - accuracy: 0.97 - ETA: 0s - loss: 0.0984 - accuracy: 0.97 - ETA: 0s - loss: 0.0985 - accuracy: 0.97 - ETA: 0s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.0988 - accuracy: 0.97 - ETA: 0s - loss: 0.0983 - accuracy: 0.97 - ETA: 0s - loss: 0.0976 - accuracy: 0.97 - ETA: 0s - loss: 0.0972 - accuracy: 0.97 - ETA: 0s - loss: 0.0976 - accuracy: 0.97 - ETA: 0s - loss: 0.0972 - accuracy: 0.97 - ETA: 0s - loss: 0.0963 - accuracy: 0.97 - ETA: 0s - loss: 0.0970 - accuracy: 0.97 - ETA: 0s - loss: 0.0971 - accuracy: 0.97 - ETA: 0s - loss: 0.0972 - accuracy: 0.97 - ETA: 0s - loss: 0.0973 - accuracy: 0.97 - ETA: 0s - loss: 0.0974 - accuracy: 0.97 - ETA: 0s - loss: 0.0970 - accuracy: 0.97 - ETA: 0s - loss: 0.0965 - accuracy: 0.97 - ETA: 0s - loss: 0.0960 - accuracy: 0.97 - ETA: 0s - loss: 0.0962 - accuracy: 0.97 - 4s 154us/sample - loss: 0.0964 - accuracy: 0.9759 - val_loss: 0.5270 - val_accuracy: 0.8062\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - ETA: 3s - loss: 0.0187 - accuracy: 1.00 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0306 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0323 - accuracy: 0.99 - ETA: 2s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0292 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0306 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0310 - accuracy: 0.99 - ETA: 1s - loss: 0.0304 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0294 - accuracy: 0.99 - ETA: 1s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0294 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0299 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 0s - loss: 0.0287 - accuracy: 0.99 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0285 - accuracy: 0.99 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - 4s 144us/sample - loss: 0.0277 - accuracy: 0.9962 - val_loss: 0.6126 - val_accuracy: 0.8069\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - ETA: 6s - loss: 0.0047 - accuracy: 1.00 - ETA: 4s - loss: 0.0066 - accuracy: 1.00 - ETA: 4s - loss: 0.0144 - accuracy: 0.99 - ETA: 3s - loss: 0.0113 - accuracy: 0.99 - ETA: 3s - loss: 0.0105 - accuracy: 0.99 - ETA: 2s - loss: 0.0157 - accuracy: 0.99 - ETA: 2s - loss: 0.0162 - accuracy: 0.99 - ETA: 2s - loss: 0.0163 - accuracy: 0.99 - ETA: 2s - loss: 0.0158 - accuracy: 0.99 - ETA: 2s - loss: 0.0146 - accuracy: 0.99 - ETA: 2s - loss: 0.0136 - accuracy: 0.99 - ETA: 2s - loss: 0.0128 - accuracy: 0.99 - ETA: 2s - loss: 0.0121 - accuracy: 0.99 - ETA: 2s - loss: 0.0126 - accuracy: 0.99 - ETA: 1s - loss: 0.0128 - accuracy: 0.99 - ETA: 1s - loss: 0.0124 - accuracy: 0.99 - ETA: 1s - loss: 0.0126 - accuracy: 0.99 - ETA: 1s - loss: 0.0121 - accuracy: 0.99 - ETA: 1s - loss: 0.0118 - accuracy: 0.99 - ETA: 1s - loss: 0.0114 - accuracy: 0.99 - ETA: 1s - loss: 0.0113 - accuracy: 0.99 - ETA: 1s - loss: 0.0116 - accuracy: 0.99 - ETA: 1s - loss: 0.0114 - accuracy: 0.99 - ETA: 1s - loss: 0.0111 - accuracy: 0.99 - ETA: 1s - loss: 0.0109 - accuracy: 0.99 - ETA: 1s - loss: 0.0123 - accuracy: 0.99 - ETA: 1s - loss: 0.0125 - accuracy: 0.99 - ETA: 1s - loss: 0.0122 - accuracy: 0.99 - ETA: 1s - loss: 0.0121 - accuracy: 0.99 - ETA: 1s - loss: 0.0121 - accuracy: 0.99 - ETA: 1s - loss: 0.0119 - accuracy: 0.99 - ETA: 1s - loss: 0.0117 - accuracy: 0.99 - ETA: 0s - loss: 0.0119 - accuracy: 0.99 - ETA: 0s - loss: 0.0117 - accuracy: 0.99 - ETA: 0s - loss: 0.0118 - accuracy: 0.99 - ETA: 0s - loss: 0.0117 - accuracy: 0.99 - ETA: 0s - loss: 0.0118 - accuracy: 0.99 - ETA: 0s - loss: 0.0122 - accuracy: 0.99 - ETA: 0s - loss: 0.0121 - accuracy: 0.99 - ETA: 0s - loss: 0.0122 - accuracy: 0.99 - ETA: 0s - loss: 0.0122 - accuracy: 0.99 - ETA: 0s - loss: 0.0121 - accuracy: 0.99 - ETA: 0s - loss: 0.0120 - accuracy: 0.99 - ETA: 0s - loss: 0.0118 - accuracy: 0.99 - ETA: 0s - loss: 0.0117 - accuracy: 0.99 - ETA: 0s - loss: 0.0117 - accuracy: 0.99 - ETA: 0s - loss: 0.0116 - accuracy: 0.99 - ETA: 0s - loss: 0.0114 - accuracy: 0.99 - ETA: 0s - loss: 0.0115 - accuracy: 0.99 - ETA: 0s - loss: 0.0113 - accuracy: 0.99 - ETA: 0s - loss: 0.0112 - accuracy: 0.99 - ETA: 0s - loss: 0.0113 - accuracy: 0.99 - ETA: 0s - loss: 0.0113 - accuracy: 0.99 - ETA: 0s - loss: 0.0112 - accuracy: 0.99 - 4s 149us/sample - loss: 0.0113 - accuracy: 0.9987 - val_loss: 0.6865 - val_accuracy: 0.8058\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - ETA: 8s - loss: 0.0024 - accuracy: 1.00 - ETA: 3s - loss: 0.0029 - accuracy: 1.00 - ETA: 3s - loss: 0.0028 - accuracy: 1.00 - ETA: 3s - loss: 0.0030 - accuracy: 1.00 - ETA: 3s - loss: 0.0050 - accuracy: 0.99 - ETA: 3s - loss: 0.0046 - accuracy: 0.99 - ETA: 3s - loss: 0.0043 - accuracy: 0.99 - ETA: 2s - loss: 0.0042 - accuracy: 0.99 - ETA: 2s - loss: 0.0052 - accuracy: 0.99 - ETA: 2s - loss: 0.0050 - accuracy: 0.99 - ETA: 2s - loss: 0.0048 - accuracy: 0.99 - ETA: 2s - loss: 0.0047 - accuracy: 0.99 - ETA: 2s - loss: 0.0052 - accuracy: 0.99 - ETA: 2s - loss: 0.0051 - accuracy: 0.99 - ETA: 2s - loss: 0.0057 - accuracy: 0.99 - ETA: 2s - loss: 0.0055 - accuracy: 0.99 - ETA: 2s - loss: 0.0053 - accuracy: 0.99 - ETA: 2s - loss: 0.0051 - accuracy: 0.99 - ETA: 2s - loss: 0.0052 - accuracy: 0.99 - ETA: 2s - loss: 0.0051 - accuracy: 0.99 - ETA: 2s - loss: 0.0050 - accuracy: 0.99 - ETA: 2s - loss: 0.0049 - accuracy: 0.99 - ETA: 2s - loss: 0.0048 - accuracy: 0.99 - ETA: 2s - loss: 0.0047 - accuracy: 0.99 - ETA: 2s - loss: 0.0052 - accuracy: 0.99 - ETA: 2s - loss: 0.0051 - accuracy: 0.99 - ETA: 2s - loss: 0.0050 - accuracy: 0.99 - ETA: 2s - loss: 0.0050 - accuracy: 0.99 - ETA: 2s - loss: 0.0049 - accuracy: 0.99 - ETA: 2s - loss: 0.0048 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0050 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0056 - accuracy: 0.99 - ETA: 1s - loss: 0.0055 - accuracy: 0.99 - ETA: 1s - loss: 0.0055 - accuracy: 0.99 - ETA: 1s - loss: 0.0054 - accuracy: 0.99 - ETA: 1s - loss: 0.0053 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0050 - accuracy: 0.99 - ETA: 1s - loss: 0.0049 - accuracy: 0.99 - ETA: 1s - loss: 0.0049 - accuracy: 0.99 - ETA: 0s - loss: 0.0052 - accuracy: 0.99 - ETA: 0s - loss: 0.0053 - accuracy: 0.99 - ETA: 0s - loss: 0.0052 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0055 - accuracy: 0.99 - ETA: 0s - loss: 0.0060 - accuracy: 0.99 - ETA: 0s - loss: 0.0059 - accuracy: 0.99 - ETA: 0s - loss: 0.0059 - accuracy: 0.99 - ETA: 0s - loss: 0.0060 - accuracy: 0.99 - ETA: 0s - loss: 0.0060 - accuracy: 0.99 - 4s 178us/sample - loss: 0.0060 - accuracy: 0.9992 - val_loss: 0.7496 - val_accuracy: 0.8014\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 10s - loss: 0.0012 - accuracy: 1.000 - ETA: 3s - loss: 0.0019 - accuracy: 1.000 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 2s - loss: 0.0018 - accuracy: 1.00 - ETA: 2s - loss: 0.0018 - accuracy: 1.00 - ETA: 2s - loss: 0.0018 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0021 - accuracy: 0.99 - ETA: 2s - loss: 0.0021 - accuracy: 0.99 - ETA: 2s - loss: 0.0021 - accuracy: 0.99 - ETA: 2s - loss: 0.0020 - accuracy: 0.99 - ETA: 2s - loss: 0.0020 - accuracy: 0.99 - ETA: 2s - loss: 0.0019 - accuracy: 0.99 - ETA: 2s - loss: 0.0019 - accuracy: 0.99 - ETA: 2s - loss: 0.0019 - accuracy: 0.99 - ETA: 2s - loss: 0.0019 - accuracy: 0.99 - ETA: 2s - loss: 0.0018 - accuracy: 0.99 - ETA: 2s - loss: 0.0018 - accuracy: 0.99 - ETA: 1s - loss: 0.0018 - accuracy: 0.99 - ETA: 1s - loss: 0.0018 - accuracy: 0.99 - ETA: 1s - loss: 0.0018 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0017 - accuracy: 0.99 - ETA: 1s - loss: 0.0016 - accuracy: 0.99 - ETA: 1s - loss: 0.0016 - accuracy: 0.99 - ETA: 1s - loss: 0.0016 - accuracy: 0.99 - ETA: 1s - loss: 0.0016 - accuracy: 0.99 - ETA: 1s - loss: 0.0020 - accuracy: 0.99 - ETA: 1s - loss: 0.0020 - accuracy: 0.99 - ETA: 1s - loss: 0.0019 - accuracy: 0.99 - ETA: 1s - loss: 0.0019 - accuracy: 0.99 - ETA: 1s - loss: 0.0019 - accuracy: 0.99 - ETA: 1s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - ETA: 0s - loss: 0.0017 - accuracy: 0.99 - ETA: 0s - loss: 0.0017 - accuracy: 0.99 - ETA: 0s - loss: 0.0017 - accuracy: 0.99 - ETA: 0s - loss: 0.0017 - accuracy: 0.99 - ETA: 0s - loss: 0.0017 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0019 - accuracy: 0.99 - ETA: 0s - loss: 0.0018 - accuracy: 0.99 - 4s 168us/sample - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.8043 - val_accuracy: 0.8051\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - ETA: 5s - loss: 7.5057e-04 - accuracy: 1.00 - ETA: 3s - loss: 6.8862e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.4330e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.3655e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.4172e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.3938e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.4821e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.3656e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.3018e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.1589e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.2128e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.2062e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.1837e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.1206e-04 - accuracy: 1.00 - ETA: 2s - loss: 7.0499e-04 - accuracy: 1.00 - ETA: 2s - loss: 6.9872e-04 - accuracy: 1.00 - ETA: 2s - loss: 6.9958e-04 - accuracy: 1.00 - ETA: 2s - loss: 6.9552e-04 - accuracy: 1.00 - ETA: 2s - loss: 6.9405e-04 - accuracy: 1.00 - ETA: 2s - loss: 6.9010e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.8669e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.8003e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.7386e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.6806e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.6547e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.6203e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.5863e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.5651e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.5334e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.4774e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.4167e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.3994e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.3889e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.3552e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.3083e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.2833e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.2869e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.2545e-04 - accuracy: 1.00 - ETA: 1s - loss: 6.2759e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.2422e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.2251e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.2158e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1978e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1708e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1360e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1177e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.0767e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.0495e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.0063e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.9777e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.9546e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.9422e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.9332e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.9124e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.8871e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.8791e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.8444e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.8189e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.8107e-04 - accuracy: 1.00 - 4s 164us/sample - loss: 5.7993e-04 - accuracy: 1.0000 - val_loss: 0.8553 - val_accuracy: 0.8062\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - ETA: 9s - loss: 4.8713e-04 - accuracy: 1.00 - ETA: 3s - loss: 3.7571e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.3788e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.4256e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.2500e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.0180e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.8924e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.7837e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.8207e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.8046e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.7484e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.7083e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.6687e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.6315e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.6180e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.5974e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.5949e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.5694e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.5387e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.5266e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.5128e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.5022e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4875e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4599e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4570e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4597e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4465e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4316e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4303e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4303e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4240e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.4035e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3883e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3894e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3830e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3735e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3514e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3484e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3441e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.3352e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3521e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3309e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3142e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3626e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3728e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3631e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3479e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3331e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3084e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.3004e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2932e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2831e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2690e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2613e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2530e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2431e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2294e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2194e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2120e-04 - accuracy: 1.00 - 4s 163us/sample - loss: 3.2078e-04 - accuracy: 1.0000 - val_loss: 0.9002 - val_accuracy: 0.8054\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - ETA: 6s - loss: 1.5219e-04 - accuracy: 1.00 - ETA: 3s - loss: 1.9786e-04 - accuracy: 1.00 - ETA: 3s - loss: 1.9607e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9561e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9855e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9997e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9956e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0198e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0465e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0119e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0059e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0041e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9985e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9829e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9893e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9735e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9599e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9606e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0097e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0222e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0139e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.0121e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.9966e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9937e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9788e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9670e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9490e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9376e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9247e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9128e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9017e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.8959e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9026e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.8972e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9124e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9082e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9012e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9038e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.8971e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.8958e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.8857e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8840e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8834e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8827e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8721e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8687e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8684e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8627e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8557e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8549e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8470e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8432e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8415e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8360e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8322e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8297e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8248e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8201e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8144e-04 - accuracy: 1.00 - 4s 164us/sample - loss: 1.8161e-04 - accuracy: 1.0000 - val_loss: 0.9419 - val_accuracy: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153cea32988>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_f, epochs = num_epochs, validation_data=(test_padded, testing_labels_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) #shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00136961,  0.0850051 ,  0.0471346 ,  0.02368701,  0.08936676,\n",
       "        0.01465387, -0.04302087, -0.13366103, -0.05391143,  0.02730086,\n",
       "        0.05496754, -0.01043492, -0.03214395, -0.04211498, -0.14496863,\n",
       "       -0.0421529 ], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vector.tsv','w',encoding='utf-8')\n",
    "out_m = io.open('meta.tsv','w',encoding='utf-8')\n",
    "for word_num in range(1,vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embedding = weights[word_num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in embedding]) + '\\n')\n",
    "out_m.close()\n",
    "out_v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
